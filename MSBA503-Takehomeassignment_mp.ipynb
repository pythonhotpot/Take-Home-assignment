{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "664b4dea-22eb-4920-98db-4428f0fed310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSBA 503 TAKE HOME ASSIGNMENT\n",
    "#NAME: MAI PHAM\n",
    "#ACKNOWLEDGEMENT: CHATGPT\n",
    "#PHOTO LINK:\n",
    "#https://www.google.com/url?sa=i&url=https%3A%2F%2Ffreshwaterpeaches.com%2Fhealthy-fruit-charcuterie-board%2F&psig=AOvVaw1ThIP5LOpH1R54CU6GuZGx&ust=1733882193304000&source=images&cd=vfe&opi=89978449&ved=0CBQQjRxqFwoTCOjGytSMnIoDFQAAAAAdAAAAABAE\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1574dac1-9cba-4bd0-8e30-d3cbd9768219",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART A\n",
    "#(i)Use two or more deep learning algorithms from the computer vision area to extract objects from a series of images and compare their performance \n",
    "#in terms of time, objects detected, and probabilities. \n",
    "#You can choose 5 or 10 images of your own and record these outputs for each of them. \n",
    "#In the end, those outcomes will be presented in a tabular format. 10 points\n",
    "\n",
    "#(ii)Can you extract anything else from these images with or without using deep learning algorithms? 4 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e60a5589-be19-4ba3-9521-d88552dcc863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\mai pham\\anaconda\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\mai pham\\anaconda\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: ultralytics in c:\\users\\mai pham\\anaconda\\lib\\site-packages (8.3.40)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\mai pham\\anaconda\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\mai pham\\anaconda\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from ultralytics) (1.11.4)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from ultralytics) (4.65.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from ultralytics) (0.12.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from ultralytics) (2.0.12)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mai pham\\anaconda\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision ultralytics opencv-python matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2482685e-f66a-477b-9ba5-d63eaddbcb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "photo1.jpg does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov8m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Using the medium YOLOv8 model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# The model is pre-trained on the COCO dataset (80 classes).\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Predict objects in the image\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphoto1.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Ensure your image path is correct\u001b[39;00m\n\u001b[0;32m     11\u001b[0m result \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Single image result\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Annotate and display the image\u001b[39;00m\n",
      "File \u001b[1;32m~\\ANACONDA\\Lib\\site-packages\\ultralytics\\engine\\model.py:557\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 557\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor(source\u001b[38;5;241m=\u001b[39msource, stream\u001b[38;5;241m=\u001b[39mstream)\n",
      "File \u001b[1;32m~\\ANACONDA\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:173\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32m~\\ANACONDA\\Lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32m~\\ANACONDA\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:231\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_model(model)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:  \u001b[38;5;66;03m# for thread-safe inference\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;66;03m# Setup source every time predict is called\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_source(source \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msource)\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;66;03m# Check if save_dir/ label file exists\u001b[39;00m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_txt:\n",
      "File \u001b[1;32m~\\ANACONDA\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:203\u001b[0m, in \u001b[0;36mBasePredictor.setup_source\u001b[1;34m(self, source)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgsz \u001b[38;5;241m=\u001b[39m check_imgsz(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mimgsz, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstride, min_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# check image size\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    202\u001b[0m )\n\u001b[1;32m--> 203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m load_inference_source(\n\u001b[0;32m    204\u001b[0m     source\u001b[38;5;241m=\u001b[39msource,\n\u001b[0;32m    205\u001b[0m     batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch,\n\u001b[0;32m    206\u001b[0m     vid_stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvid_stride,\n\u001b[0;32m    207\u001b[0m     buffer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mstream_buffer,\n\u001b[0;32m    208\u001b[0m )\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39msource_type\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mstream\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mscreenshot\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# many images\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo_flag\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;28;01mFalse\u001b[39;00m]))\n\u001b[0;32m    215\u001b[0m ):  \u001b[38;5;66;03m# videos\u001b[39;00m\n",
      "File \u001b[1;32m~\\ANACONDA\\Lib\\site-packages\\ultralytics\\data\\build.py:202\u001b[0m, in \u001b[0;36mload_inference_source\u001b[1;34m(source, batch, vid_stride, buffer)\u001b[0m\n\u001b[0;32m    200\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m LoadPilAndNumpy(source)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 202\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m LoadImagesAndVideos(source, batch\u001b[38;5;241m=\u001b[39mbatch, vid_stride\u001b[38;5;241m=\u001b[39mvid_stride)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# Attach source types to the dataset\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28msetattr\u001b[39m(dataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, source_type)\n",
      "File \u001b[1;32m~\\ANACONDA\\Lib\\site-packages\\ultralytics\\data\\loaders.py:341\u001b[0m, in \u001b[0;36mLoadImagesAndVideos.__init__\u001b[1;34m(self, path, batch, vid_stride)\u001b[0m\n\u001b[0;32m    339\u001b[0m         files\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m((parent \u001b[38;5;241m/\u001b[39m p)\u001b[38;5;241m.\u001b[39mabsolute()))  \u001b[38;5;66;03m# files (relative to *.txt file parent)\u001b[39;00m\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 341\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;66;03m# Define files as images or videos\u001b[39;00m\n\u001b[0;32m    344\u001b[0m images, videos \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: photo1.jpg does not exist"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8m.pt\")  # Using the medium YOLOv8 model\n",
    "# The model is pre-trained on the COCO dataset (80 classes).\n",
    "\n",
    "# Predict objects in the image\n",
    "results = model.predict(\"photo1.jpg\")  # Ensure your image path is correct\n",
    "result = results[0]  # Single image result\n",
    "\n",
    "# Annotate and display the image\n",
    "annotated_image = result.plot()  # Annotate the image\n",
    "plt.imshow(annotated_image)\n",
    "plt.axis('off')  # Hide axes for better visualization\n",
    "plt.show()\n",
    "\n",
    "# Save the annotated image\n",
    "annotated_image_pil = Image.fromarray(annotated_image)\n",
    "annotated_image_pil.save(\"detected_objects_yolo_photo1.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a0af3f4-28f6-4032-a30b-56563c24b0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in current directory:\n",
      "['-Ya83EHCSt-GvNxBworfvQ_f235b6c002314f50a201eae236c5dd74_Module-4-Code-Files.zip', '.ipynb_checkpoints', '1d37ea8d-89a0-499e-80f2-5ec34e357215 (1).JPG', '1d37ea8d-89a0-499e-80f2-5ec34e357215.JPG', '2021 F1  Season.twbx', '280422343.zip', '502-downloads', '503-downloads', '504-downloads', '506-downloads', '507-downloads', 'Age vs Military Affiliation.png', 'Anaconda3-2024.02-1-Windows-x86_64.exe', 'Anaconda3-2024.10-1-Windows-x86_64.exe', 'apartment+for+rent+classified', 'apartment+for+rent+classified.zip', 'Artificial_Neural_Network', 'Assignment 1 (1).pdf', 'Assignment 3_MaiPham.ipynb', 'Assingment_2_MP (1).ipynb', 'Assingment_2_MP (2).ipynb', 'Assingment_2_MP.ipynb', 'business processs.png', 'Case - Blitz Marketing Campaign (student).pptx', 'Class analysis template.pptx', 'Complete_with_Docusign_Mai_Pham_Award_Letter.zip', 'Continuous random variables.xlsx', 'Correlation Example.xlsx', 'DB.Browser.for.SQLite-v3.13.0-win64.msi', 'desktop.ini', 'download.ics', 'Elearning6_MP-1.png', 'employee_small_resources', 'Exam files for students', 'Exam files for students.zip', 'Exam practice problems.pdf', 'Exam practice problems.twbx', 'Facility Performance Comparison - Copy.xlsx', 'Facility performance comparison.docx', 'Facility Performance Comparison.pptx', 'Facility Performance Comparison.xlsx', 'FILE_4417.pdf', 'final_p2_resources.zip', 'Flair Furniture.xlsx', 'Genetech data.xlsx', 'GenetechAssignment_Pham_Ratchatapongsiri (1).pptx', 'GenetechAssignment_Pham_Ratchatapongsiri.pptx', 'GitHubDesktopSetup-x64.exe', 'Grade data.xlsx', 'Great Resignation or Great Awakening.twbx', 'Group Case Study_Four Wheel Tire Shop -1.docx', 'Group_casestudy_Rai_Pham.xlsx', 'Group_casestudy_Report_Rai_Pham.pdf', 'hot-pot-filled-with-broth.jpg', 'HotPot.jpg', 'HW5_Resource_Files.zip', 'Illegal dumping.ipynb', 'Image_Analytics', 'Image_Analytics.zip', 'imdb2015.zip', 'IMG_9365.jpg', 'IMG_9909.JPG', 'In-Class Assignment 2', 'In-Class Assignment 2.zip', 'InClassAssignment1_MaiPham.ipynb', 'Initial analysis.twbx', 'Interpreting Art _ Iron Viz 2022 _ Drawful 2.twbx', 'Introduction - Tagged.pdf', 'Labor data (full analysis).xlsx', 'Lanford Sub Shop (1).xlsx', 'Lanford Sub Shop.xlsx', 'LetterofIntro_MP_20241024.pdf', 'McAfee_Installer_serial_riAOk1broOX3NYuRh6EKXg2_key_affid_1357_akey.exe', 'Merged work log.xlsx', 'MicrosoftWindows.Client.CBS_cw5n1h2txyewy!InputApp', 'MSBA 502 Project Presentation.pdf', 'MSBA 503 Take Home Assignment.docx', 'MSBA 507 Lecture 1105 Yang.pdf', 'MSBA 507 Lecture 1112 Yang.pdf', 'MSBA 507 Lecture 1119 ML Part 1 Yang.pdf', 'MSBA 507 Lecture 1121 ML Part 2 Yang.pdf', 'MSBA 507 Lecture 1203 Yang.pdf', 'MSBA503-Takehomeassignment_mp.ipynb', 'MSBA505_PlantEfficiency_MP_20241208.pptx', 'MSBA_500_Introduction_to_Data_Analytics_Lecture Slides.pdf', 'MSBA_503_01_Analytics_Programming_II_Using_AI_for_Coding.pdf', 'MyCard.pdf', 'Norpan case.xlsx', 'OneDrive_2023 Data.zip', 'OneDrive_2024-Data.zip', 'postgresql-16.4-1-windows-x64.exe', 'Professional Development & Exploration (1).pdf', 'project2_resources.zip', 'Project3_Custom_Query_Template.pdf', 'project3_resources.zip', 'project_2_sql_submission.sql', 'pycharm-community-2024.1.3.exe', 'python-3.12.4-amd64.exe', 'qF2iRJxURay0_Amp-Z_wnQ_bb0e60980e0542d7be121f3e934634f1_Mod-4-Slide-PDF-Notetaking-Handout-Updated.zip', 'Resume_MaiPham_202407.pdf', 'Resume_MaiPham_MSBA2425_202407 - updated.docx', 'Resume_PhamMai_202412.pdf', 'Scraping (Lecture slide and code).zip', 'Simulation Basics - Tagged.pdf', 'Simulation Basics.pptx', 'Simulation exercise (excel).xlsx', 'SSRacknowledgment_TrungPham_20241005.pdf', 'Story telling.pptx', 'submission copy.yaml', 'Super Store Analysis.pptx', 'Super store assignment.twbx', 'Super store class data-1.xlsx', 'Super store class data-2.xlsx', 'Super store class data.xlsx', 'Super store data.xlsx', 'Tableau 1.twbx', 'Tableau 2.twbx', 'Tableau 4 (full).twbx', 'TableauAutoUpdate', 'TableauDesktop-64bit-2024-2-2.exe', 'Takehome assignment', 'Teams (1).xlsx', 'Teams.xlsx', 'Text analysis (slides, data, and notebooks)', 'updated_volunteer_data.xlsx', 'updated_work_log.xlsx', 'USD logo.ico', 'USD MSBA 501-Project Guidelines (1).pdf', 'USD MSBA 501-Project Guidelines.pdf', 'USD MSBA 503-Project and Draft Guidelines.pdf', 'Volunteer Application.xlsx', 'Volunteer2023_worklog-updated.xlsx', 'VSCodeUserSetup-x64-1.93.1.exe', 'WESTPORT APPLICATION FOR ARCHITECTURALLANDSCAPE IMPROVEMENTS.pdf', 'Work Log.xlsx', 'WORK_LOG_CLEAN.xlsx', 'yolov8m.pt', 'yolov8n.pt', 'ZoomInstallerFull.exe', 'Zoom_cm_f4sfb5kxvZ9vvrZo4_myVr27B7QHLvV4Rd6Hj5ZmHXkM9MnIdNnf9+k@PGBRS5pMJ5Gg+RfN_k90e3422d18a75bec_.exe', '_Volunteer Post-Event Feedback Survey (Responses).xlsx', '~$_Volunteer Post-Event Feedback Survey (Responses).xlsx', '~Great Resignation or Great Awakening__19236.twbr', '~Initial analysis__8568.twbr', '~Interpreting Art _ Iron Viz 2022 _ Drawful 2__25084.twbr', '~Tableau 4 (full)__27012.twbr']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Files in current directory:\")\n",
    "print(os.listdir(os.getcwd()))  # Lists all files in the current directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edb0fed6-7315-4332-b0ce-55c93f608abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mai Pham\\ANACONDA\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mai Pham\\ANACONDA\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Faster R-CNN model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = \"195.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "image_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "# Predict objects\n",
    "outputs = model(image_tensor)[0]  # Get prediction results\n",
    "\n",
    "# Annotate and display the image\n",
    "fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "ax.imshow(image)\n",
    "for box, score in zip(outputs['boxes'], outputs['scores']):\n",
    "    if score > 0.5:  # Confidence threshold\n",
    "        x_min, y_min, x_max, y_max = box.tolist()\n",
    "        rect = patches.Rectangle(\n",
    "            (x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "            linewidth=2, edgecolor='r', facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "ax.axis('off')  # Hide axes for better visualization\n",
    "plt.show()\n",
    "\n",
    "# Save the annotated image\n",
    "fig.savefig(\"detected_objects_faster_rcnn_195.jpg\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c069e163-a8f8-4664-a21b-81bba4323d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af109efb-2cae-4f18-a025-d7217975dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART B\n",
    "#Upload the code to GitHub, create a nice Read Me file, and attach the link to the Word document. \n",
    "#Make sure the code in GitHub is well-commented. You do not need to upload the images on GitHub. \n",
    "#The coding file should have the code and outputs printed. 6 points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54d78dc-d971-4290-acae-a39b48b0ac10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
